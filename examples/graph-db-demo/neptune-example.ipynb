{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neptune as Graph Memory\n",
    "\n",
    "In this notebook, we will be connecting using a Amazon Neptune Analytics instance as our memory graph storage for Mem0.\n",
    "\n",
    "The Graph Memory storage persists memories in a graph or relationship form when performing `m.add` memory operations. It then uses vector distance algorithms to find related memories during a `m.search` operation. Relationships are returned in the result, and add context to the memories.\n",
    "\n",
    "Reference: [Vector Similarity using Neptune Analytics](https://docs.aws.amazon.com/neptune-analytics/latest/userguide/vector-similarity.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### 1. Install Mem0 with Graph Memory support \n",
    "\n",
    "To use Mem0 with Graph Memory support, install it using pip:\n",
    "\n",
    "```bash\n",
    "pip install \"mem0ai[graph]\"\n",
    "```\n",
    "\n",
    "This command installs Mem0 along with the necessary dependencies for graph functionality.\n",
    "\n",
    "### 2. Connect to Neptune\n",
    "\n",
    "To connect to Amazon Neptune Analytics, you need to configure Neptune with your Amazon profile credentials. The best way to do this is to declare environment variables with IAM permission to your Neptune Analytics instance. The `graph-identifier` for the instance to persist memories needs to be defined in the Mem0 configuration under `\"graph_store\"`, with the `\"neptune\"` provider.  Note that the Neptune Analytics instance needs to have `vector-search-configuration` defined to meet the needs of the llm model's vector dimensions, see: https://docs.aws.amazon.com/neptune-analytics/latest/userguide/vector-index.html.\n",
    "\n",
    "```python\n",
    "embedding_dimensions = 1536\n",
    "graph_identifier = \"<MY-GRAPH>\" # graph with 1536 dimensions for vector search\n",
    "config = {\n",
    "    \"embedder\": {\n",
    "        \"provider\": \"openai\",\n",
    "        \"config\": {\n",
    "            \"model\": \"text-embedding-3-large\",\n",
    "            \"embedding_dims\": embedding_dimensions\n",
    "        },\n",
    "    },\n",
    "    \"graph_store\": {\n",
    "        \"provider\": \"neptune\",\n",
    "        \"config\": {\n",
    "            \"endpoint\": f\"neptune-graph://{graph_identifier}\",\n",
    "        },\n",
    "    },\n",
    "}\n",
    "```\n",
    "\n",
    "### 3. Configure OpenSearch\n",
    "\n",
    "We're going to use OpenSearch as our vector store.  You can run [OpenSearch from docker image](https://docs.opensearch.org/docs/latest/install-and-configure/install-opensearch/docker/):\n",
    "\n",
    "```bash\n",
    "docker pull opensearchproject/opensearch:2\n",
    "```\n",
    "\n",
    "And verify that it's running with a `<custom-admin-password>`:\n",
    "\n",
    "```bash\n",
    " docker run -d -p 9200:9200 -p 9600:9600 -e \"discovery.type=single-node\" -e \"OPENSEARCH_INITIAL_ADMIN_PASSWORD=<custom-admin-password>\" opensearchproject/opensearch:latest\n",
    "\n",
    " curl https://localhost:9200 -ku admin:<custom-admin-password>\n",
    "```\n",
    "\n",
    "We're going to connect [OpenSearch using the python client](https://github.com/opensearch-project/opensearch-py):\n",
    "\n",
    "```bash\n",
    "pip install \"opensearch-py\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Do all the imports and configure OpenAI (enter your OpenAI API key):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mem0 import Memory\n",
    "from langchain_aws import NeptuneAnalyticsGraph\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# logging.getLogger(\"mem0.memory.neptune_memory\").setLevel(logging.DEBUG)\n",
    "# logging.getLogger(\"mem0.memory.neptune_base\").setLevel(logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    stream=sys.stdout,  # Explicitly set output to stdout\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the Mem0 configuration using:\n",
    "- openai as the embedder\n",
    "- Amazon Neptune Analytics instance as a graph store\n",
    "- OpenSearch as the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_identifier = os.environ.get(\"GRAPH_ID\")\n",
    "opensearch_username = os.environ.get(\"OS_USERNAME\")\n",
    "opensearch_password = os.environ.get(\"OS_PASSWORD\")\n",
    "config = {\n",
    "    \"embedder\": {\n",
    "        \"provider\": \"openai\",\n",
    "        \"config\": {\"model\": \"text-embedding-3-large\", \"embedding_dims\": 1536},\n",
    "    },\n",
    "    \"graph_store\": {\n",
    "        \"provider\": \"neptune\",\n",
    "        \"config\": {\n",
    "            \"endpoint\": f\"neptune-graph://{graph_identifier}\",\n",
    "        },\n",
    "    },\n",
    "    \"vector_store\": {\n",
    "        \"provider\": \"opensearch\",\n",
    "        \"config\": {\n",
    "            \"collection_name\": \"vector_store\",\n",
    "            \"host\": \"localhost\",\n",
    "            \"port\": 9200,\n",
    "            \"user\": opensearch_username,\n",
    "            \"password\": opensearch_password,\n",
    "            \"use_ssl\": True,\n",
    "            \"verify_certs\": False,\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Memory initializiation\n",
    "\n",
    "Initialize Memgraph as a Graph Memory store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Memory.from_config(config_dict=config)\n",
    "\n",
    "app_id = \"movies\"\n",
    "user_id = \"alice\"\n",
    "\n",
    "m.delete_all(user_id=user_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store memories\n",
    "\n",
    "Create memories and store one at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Store inferred memories (default behavior)\n",
    "result = m.add(messages, user_id=user_id, metadata={\"category\": \"movie_recommendations\"})\n",
    "\n",
    "all_results = m.get_all(user_id=user_id)\n",
    "for n in all_results[\"results\"]:\n",
    "    print(f\"node \\\"{n[\"memory\"]}\\\": [hash: {n[\"hash\"]}]\")\n",
    "\n",
    "for e in all_results[\"relations\"]:\n",
    "    print(f\"edge \\\"{e[\"source\"]}\\\" --{e[\"relationship\"]}--> \\\"{e[\"target\"]}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"How about a thriller movies? They can be quite engaging.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Store inferred memories (default behavior)\n",
    "result = m.add(messages, user_id=user_id, metadata={\"category\": \"movie_recommendations\"})\n",
    "\n",
    "all_results = m.get_all(user_id=user_id)\n",
    "for n in all_results[\"results\"]:\n",
    "    print(f\"node \\\"{n[\"memory\"]}\\\": [hash: {n[\"hash\"]}]\")\n",
    "\n",
    "for e in all_results[\"relations\"]:\n",
    "    print(f\"edge \\\"{e[\"source\"]}\\\" --{e[\"relationship\"]}--> \\\"{e[\"target\"]}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Store inferred memories (default behavior)\n",
    "result = m.add(messages, user_id=user_id, metadata={\"category\": \"movie_recommendations\"})\n",
    "\n",
    "all_results = m.get_all(user_id=user_id)\n",
    "for n in all_results[\"results\"]:\n",
    "    print(f\"node \\\"{n[\"memory\"]}\\\": [hash: {n[\"hash\"]}]\")\n",
    "\n",
    "for e in all_results[\"relations\"]:\n",
    "    print(f\"edge \\\"{e[\"source\"]}\\\" --{e[\"relationship\"]}--> \\\"{e[\"target\"]}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Store inferred memories (default behavior)\n",
    "result = m.add(messages, user_id=user_id, metadata={\"category\": \"movie_recommendations\"})\n",
    "\n",
    "all_results = m.get_all(user_id=user_id)\n",
    "for n in all_results[\"results\"]:\n",
    "    print(f\"node \\\"{n[\"memory\"]}\\\": [hash: {n[\"hash\"]}]\")\n",
    "\n",
    "for e in all_results[\"relations\"]:\n",
    "    print(f\"edge \\\"{e[\"source\"]}\\\" --{e[\"relationship\"]}--> \\\"{e[\"target\"]}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = m.search(\"what does alice love?\", user_id=user_id)\n",
    "for result in search_results[\"results\"]:\n",
    "    print(f\"\\\"{result[\"memory\"]}\\\" [score: {result[\"score\"]}]\")\n",
    "for relation in search_results[\"relations\"]:\n",
    "    print(f\"{relation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.delete_all(\"user_id\")\n",
    "m.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
